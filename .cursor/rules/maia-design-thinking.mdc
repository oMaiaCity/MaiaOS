---
description: Design Thinking-based planning workflow for structuring new intents and features. Follow this process whenever planning a new feature, refactor, or system change.
alwaysApply: false
---

# MaiaOS Planning Workflow: Design Thinking Process

You are a planning assistant that uses **Design Thinking** principles to structure planning workflows. When a user wants to plan a new intent, feature, or system change, follow this structured process.

## Core Principles

**Design Thinking is:**
- **Human-centered**: Focus on the people (users, developers, stakeholders) who will interact with the solution
- **Iterative**: Don't rush to a solution; iterate and refine based on feedback
- **Non-linear**: Stages can be revisited, run in parallel, or skipped as needed
- **Problem-focused**: Solve the right problem, not just any problem
- **System-aware**: Recognize everything is interconnected; can't solve one piece in isolation

**End Goal**: Create solutions that are **Desirable** (meets user needs), **Feasible** (technically possible), and **Viable** (sustainable/maintainable).

## The 6-Stage Process

MaiaOS uses a 6-stage design thinking process: **Empathize ‚Üí Define ‚Üí Ideate ‚Üí Prototype ‚Üí Implement ‚Üí Review & Feedback**

### Stage 1: EMPATHIZE ‚Äî Understand the Context

**Goal**: Deeply understand the problem, users, and constraints before proposing solutions.

**Questions to Explore:**
- Who is affected by this problem? (users, developers, maintainers)
- What is the current state? What's working? What's broken?
- What are the real needs vs. stated wants?
- What constraints exist? (technical, time, resources, architecture)
- What assumptions are we making? Challenge them.

**Output**: A clear understanding of the problem space, user needs, and constraints.

**LLM Action**: Ask clarifying questions, read relevant code/files, understand the codebase context.

---

### Stage 2: DEFINE ‚Äî State the Problem Clearly

**Goal**: Synthesize insights into a clear problem statement and success criteria.

**Activities:**
- Reframe the problem from multiple perspectives
- Define what "done" looks like (success criteria)
- Identify what's out of scope
- Create a problem statement: "How might we [solve X] so that [users benefit Y]?"

**Output**: 
- Clear problem statement
- Success criteria (desirable, feasible, viable)
- Scope boundaries

**LLM Action**: Synthesize findings into a concise problem definition and success metrics.

---

### Stage 3: IDEATE ‚Äî Generate Solutions

**Goal**: Explore multiple solution approaches without premature commitment.

**Activities:**
- Brainstorm multiple approaches (divergent thinking)
- Consider edge cases and alternatives
- Evaluate trade-offs (simplicity vs. flexibility, speed vs. robustness)
- Challenge assumptions: "What if we did the opposite?"

**Output**: 
- 2-3 viable solution approaches
- Pros/cons of each approach
- Recommended approach with rationale

**LLM Action**: Propose multiple solution paths, analyze trade-offs, recommend the best fit.

---

### Stage 4: PROTOTYPE ‚Äî Design the Solution

**Goal**: Create a concrete, testable plan that can be validated.

**Activities:**
- Break solution into implementable steps
- Define file structure, API changes, data models
- Identify dependencies and integration points
- Create a phased implementation plan (MVP ‚Üí full solution)
- Consider backward compatibility and migration paths

**Output**: 
- Detailed implementation plan with steps
- File structure and code organization
- API/interface definitions
- Testing strategy
- Rollout plan (if applicable)

**LLM Action**: Create a structured plan document with todos, file paths, code structure, and implementation steps.

---

### Stage 5: IMPLEMENT ‚Äî Execute the Solution

**Goal**: Execute the plan incrementally using TDD, with human-in-the-loop validation at each milestone.

#### Implementation Planning Structure

**1. Incremental Milestones**

Structure the implementation as **incremental, least invasive, testable, migrateable mini milestones**:

- ‚úÖ **Each milestone must be independently testable**
- ‚úÖ **Each milestone should be fully functional** (not half-done features)
- ‚úÖ **Migrate 100% fullstack** - no partial migrations or legacy wrappers
- ‚ùå **Never create hacky backwards compatibility layers**
- ‚ùå **Never create legacy deprecation shims**
- üîÑ **Call human into loop after finishing ONE milestone to test**

**Example Milestone Structure:**
```
Milestone 1: Add base engine structure + tests
  ‚îú‚îÄ Write tests (Red)
  ‚îú‚îÄ Implement (Green)
  ‚îú‚îÄ Clean up legacy & migrate 100%
  ‚îî‚îÄ Pause for human feedback ‚úã

Milestone 2: Implement core methods + tests
  ‚îú‚îÄ Write tests (Red)
  ‚îú‚îÄ Implement (Green)
  ‚îú‚îÄ Clean up legacy & migrate 100%
  ‚îî‚îÄ Pause for human feedback ‚úã

Milestone 3: Add integration tests + documentation
  ‚îú‚îÄ Write tests (Red)
  ‚îú‚îÄ Implement (Green)
  ‚îú‚îÄ Clean up legacy & migrate 100%
  ‚îî‚îÄ Pause for human feedback ‚úã
```

**2. Test-Driven Development (TDD) - ALWAYS**

For **every** implementation milestone, follow this strict order:

**Step 1: Write Tests First** üß™
- Write unit tests for the functionality you're about to implement
- Write integration tests if applicable
- Tests should fail initially (Red phase)

**Step 2: Implement the Solution** üíª
- Write the minimal code to make tests pass
- Refactor for clarity and maintainability
- Ensure all tests pass (Green phase)

**Step 3: Clean Up Legacy & Migrate 100%** üßπ
- **ALWAYS migrate 100% fullstack** - no partial migrations
- Remove ALL legacy code that has been replaced
- Delete backwards compatibility hacks/shims/wrappers
- Remove deprecated functions and deprecated API patterns
- Ensure new architecture is fully adopted everywhere
- Update ALL call sites to use new patterns
- Delete unused files, functions, and old implementations
- Verify no "legacy mode" flags or switches remain
- **ELIMINATE ALL MOCKS** - Replace with real implementations

**Critical Cleanup Checklist:**
- ‚úÖ All old implementations deleted (not commented out)
- ‚úÖ All backwards compatibility layers removed
- ‚úÖ All call sites updated to new API/architecture
- ‚úÖ All tests updated to test new implementation only
- ‚úÖ No "if (legacy)" or "if (oldMode)" conditionals
- ‚úÖ All imports/exports updated to new structure
- ‚ùå No "TODO: migrate this later" comments
- ‚ùå No "keep for backwards compatibility" code

**CRITICAL: Zero Mocks Checklist** üö´

Mocks hide bugs and create false confidence. Replace ALL mocks with real implementations:

**Search and Destroy Mocks:**
```bash
# 1. Search for mock classes
grep -r "class Mock" src/ --include="*.js"
grep -r "MockRaw" src/ --include="*.js"

# 2. Search for stub/fake patterns
grep -r "stub\|fake\|spy" src/ --include="*.js"

# 3. Search for mock imports
grep -r "sinon\|jest.mock\|vi.mock" src/ --include="*.js"

# ALL results MUST be 0 (except comments about mocking policy)
```

**Replace Mocks With Real Implementations:**
- ‚úÖ Use real database connections (not in-memory fakes)
- ‚úÖ Use real API clients (not stubbed responses)
- ‚úÖ Use real CRDTs from actual packages (not MockRawCoMap)
- ‚úÖ Initialize real LocalNode/context (not fake setup)
- ‚úÖ Generate real IDs (not "test_123" or "fake_id")
- ‚ùå NO Mock classes (MockRawCoMap, MockLocalNode, FakeStorage)
- ‚ùå NO Stub methods (stub.returns(), fake implementations)
- ‚ùå NO In-memory fakes (FakeDatabase, MockAPI)
- ‚ùå NO Hardcoded test IDs (use real ID generators)

**Why Zero Mocks Matters:**
1. **Mocks hide bugs**: Real systems have timing, race conditions, edge cases that mocks can't simulate
2. **False confidence**: Tests pass with mocks but fail in production
3. **Brittle tests**: Mocks require maintenance when real implementation changes
4. **Integration issues**: Mocks don't catch API contract violations
5. **Production parity**: Tests should mirror production environment

**Acceptable Testing Patterns:**
- ‚úÖ Real test databases (SQLite in-memory, test fixtures)
- ‚úÖ Real service instances with test configuration
- ‚úÖ Real network calls to test/staging environments
- ‚úÖ Real CRDT initialization (LocalNode, Groups)
- ‚úÖ Integration tests with actual data files

**When Mocks Are Acceptable (Rare)**:
- ‚úÖ External services you don't control (payment gateways, 3rd party APIs)
- ‚úÖ Slow operations in unit tests (use integration tests for real versions)
- ‚úÖ Rate-limited APIs (but have integration tests with real API)

**Validation Commands (Run After Every Milestone):**
```bash
# Scan entire src/ directory
cd [project-root]

# Search for mocks (MUST return 0)
grep -r "class Mock" src/ --include="*.js" | wc -l
grep -r "Mock" src/**/*.test.js | wc -l

# Search for stubs/fakes (MUST return 0)  
grep -r "stub\|fake" src/ --include="*.js" | grep -v "fakeId\|// fake" | wc -l

# Verify real implementation imports
grep -r "from ['\"]<real-package>" src/ --include="*.js" | wc -l

# Run full test suite (MUST pass with real implementations)
bun test
```

**If Mocks Found:**
1. **STOP** - Do not proceed to next milestone
2. Identify what the mock is simulating
3. Replace with real implementation:
   - Import real package/service
   - Initialize real context (LocalNode, database, etc.)
   - Use real methods and data
4. Update tests to work with real implementation
5. Re-run validation commands
6. Only proceed when all mocks eliminated

**Step 4: Manual Browser Debugging** üåê

For **frontend features, UI components, or web-based functionality**, use **Cursor's native in-app browser** for **manual debugging and verification** during implementation.

**What This Is:**
- ‚úÖ **Manual debugging** - LLM opens browser, inspects console, checks network, verifies functionality
- ‚úÖ **Implementation verification** - Quick checks during development to ensure code works
- ‚úÖ **Anti-mock validation** - Verify real data (not mocks) appears in console logs

**What This Is NOT:**
- ‚ùå **NOT automated browser tests** (no Puppeteer/Playwright/Selenium scripts)
- ‚ùå **NOT test automation** (write unit/integration tests instead)
- ‚ùå **NOT end-to-end test suites** (manual verification only)

**Browser Debugging Workflow:**

1. **Start the dev server** (if not already running):
   ```bash
   bun dev:app  # or appropriate service command
   ```

2. **Open Cursor's Browser:**
   - Open browser in Cursor IDE (browser icon or Command Palette)
   - Navigate to app URL (e.g., `http://localhost:4202`)

3. **Quick Visual Check:**
   - Does the UI render?
   - Are there obvious visual bugs?
   - Do basic interactions work (click buttons, fill forms)?

4. **Console Check (CRITICAL for Anti-Mock Validation):**
   - Open DevTools Console
   - **Look for**:
     - ‚úÖ Real IDs: `co_z...` (not `mock_123`, `test_`, `fake_`)
     - ‚úÖ Real data structures (not hardcoded test data)
     - ‚úÖ No JavaScript errors or unexpected warnings
   
   **Example:**
   ```javascript
   // ‚úÖ Good - Real data in console
   console.log("Created blog:", "co_z9h5nwiNynbxnC3nTw...")
   
   // ‚ùå Bad - Mocked data in console  
   console.log("Created blog:", "mock_blog_123")
   ```

5. **Network Tab Check:**
   - Are requests succeeding (200-299)?
   - Any failed requests (4xx, 5xx)?
   - Do payloads look correct?

6. **Basic Interaction Testing:**
   - Click through the feature
   - Verify core functionality works
   - Check for obvious errors

7. **Document Findings:**
   - Note any console errors
   - Note any network failures
   - Note any visual bugs
   - Confirm real data (not mocks) if applicable

**When to Use Browser Debugging:**
- ‚úÖ After implementing frontend features (quick verification)
- ‚úÖ When suspicious of mocks (check console for real data)
- ‚úÖ When UI isn't rendering as expected
- ‚úÖ When investigating bugs or errors
- ‚úÖ During milestone checkpoints (verify everything still works)

**When NOT to Use:**
- ‚ùå For backend-only changes (no UI impact)
- ‚ùå As a replacement for unit tests (write proper tests!)
- ‚ùå For automated testing (this is manual debugging only)

**Browser Debugging Best Practices:**
- **Quick checks, not thorough testing** - This is debugging, not QA
- **Console is your friend** - Most issues show up here first
- **Real data validation** - Console should show real IDs, not mocks
- **Don't write browser automation** - Just open browser, look around, document findings
- **Complement tests, don't replace them** - Unit/integration tests are still required

**Step 5: Human-in-the-Loop Feedback** üë§
- Pause after milestone completion
- Run all tests (unit, integration, and browser) to verify nothing broke
- Present results (including screenshots and console logs) to human for feedback
- Iterate if needed before next milestone

**Testing Requirements for OS-Level Components:**

For OS-level engines (like `style-engine`, `state-engine`, etc.), always structure tests as:

```
engines/
‚îú‚îÄ‚îÄ [engine-name]/
‚îÇ   ‚îú‚îÄ‚îÄ [engine-name].engine.js           ‚Üê Implementation
‚îÇ   ‚îú‚îÄ‚îÄ [engine-name].engine.test.js      ‚Üê Unit tests (colocated)
‚îÇ   ‚îî‚îÄ‚îÄ [engine-name].engine.integration.test.js  ‚Üê Integration tests
```

**Unit Tests** (`*.engine.test.js`):
- Test individual methods in isolation
- Use mocks/stubs for dependencies
- Fast, focused tests

**Integration Tests** (`*.engine.integration.test.js`):
- Test with real files/data from `vibes/` folder
- Auto-discover test cases (e.g., all `.style.maia` files)
- Validate end-to-end workflows
- Use standard Bun test output (no custom summaries)

**Test Output Style:**
- Use standard Bun test runner output
- Let Bun handle test reporting naturally
- Use `--bail` flag to stop on first error (optional)

**Example Test Structure:**
```javascript
// Integration test - auto-discovery
const files = discoverFiles(vibesDir);

for (const file of files) {
  describe(`${file.vibe}/${file.name}`, () => {
    it('validates structure and generates valid CSS', () => {
      // Test structure validation
      // Test CSS generation
      // Test output quality
    });
  });
}
```

**Output**: 
- Working, tested code at each milestone
- Human feedback incorporated before proceeding
- All tests passing (green)

**LLM Action**: 
1. Write tests first (Red phase)
2. Implement solution to pass tests (Green phase)
3. Clean up ALL legacy code and ensure 100% migration (Refactor phase)
4. Run browser-based tests (for frontend features):
   - Navigate to app, take screenshots, check console, test interactions
5. Pause for human feedback
6. Iterate based on feedback
7. Move to next milestone

---

### Stage 6: REVIEW & FEEDBACK ‚Äî Validate and Document

**Goal**: Ensure solution is complete, tested, and documented before considering it "done."

#### Review Process

**1. Verify All Tests Pass & No Legacy Code Remains** ‚úÖ

Before proceeding, ensure:
- ‚úÖ All new tests pass
- ‚úÖ No existing tests are broken
- ‚úÖ Integration tests pass with real data
- ‚úÖ Browser-based tests pass (for frontend features)
- ‚úÖ No linter errors
- ‚úÖ Code follows project conventions
- ‚úÖ ALL legacy code has been deleted (no commented out code)
- ‚úÖ No backwards compatibility hacks remain
- ‚úÖ 100% migration to new architecture complete
- ‚úÖ No "TODO: migrate later" comments exist

**Run full test suite:**
```bash
bun test
```

**Manual Browser Debugging** üåê

For frontend features, use **Cursor's browser** for **quick manual verification** (not automated testing):

1. **Open Cursor Browser:**
   - Open browser in Cursor IDE
   - Navigate to app URL (e.g., `http://localhost:4202`)

2. **Quick Visual Check:**
   - Does UI render correctly?
   - Any obvious visual bugs?
   - Basic interactions working?

3. **Console Check (CRITICAL - Anti-Mock Validation):**
   - Open DevTools Console
   - **Check for**:
     - ‚úÖ Real data: `co_z...` IDs (not `mock_123`)
     - ‚úÖ No JavaScript errors
     - ‚úÖ No unexpected warnings
   - **Example:**
     ```
     // ‚úÖ Good - Real data
     Created blog: co_z9h5nwiNynbxnC3nTwPMPkrVaMQ
     
     // ‚ùå Bad - Mocked data (STOP!)
     Created blog: mock_blog_123
     ```

4. **Network Tab Check:**
   - Open Network tab
   - **Quick checks**:
     - ‚úÖ Requests succeed (200-299)?
     - ‚ùå Any failures (4xx, 5xx)?
     - ‚úÖ Payloads look correct?

5. **Basic Interaction Check:**
   - Click through core feature
   - Verify basic functionality
   - Test a few user actions

6. **Document Findings:**
   - Note console errors (if any)
   - Note network issues (if any)
   - Confirm real data visible (no mocks)

**Browser Debugging Output:**
- ‚úÖ "Console: 0 errors, real co-ids visible"
- ‚úÖ "Network: requests succeed, no failures"  
- ‚úÖ "UI: renders correctly, interactions work"
- OR
- ‚ùå "Console: Found mock IDs (mock_123) - STOP and fix!"
- ‚ùå "Network: requests failing - investigate"
- ‚ùå "UI: visual bugs found - fix before proceeding"

**Red Flags (STOP and Fix)**:
- ‚ùå Console shows `mock_`, `test_`, `fake_` in IDs
- ‚ùå Console shows `undefined` for critical data
- ‚ùå JavaScript errors in console
- ‚ùå Network requests failing
- ‚ùå UI not rendering or broken

**Remember**: This is **manual debugging**, NOT automated browser testing. Just open browser, look around, document findings. Write unit/integration tests separately.

**2. Human-in-the-Loop Final Feedback** üë§

Present to human:
- Summary of changes
- Test results (all passing)
- Any trade-offs or limitations
- Next steps (if any)

**Questions to ask:**
- Does the solution meet the original requirements?
- Are there any edge cases we missed?
- Is the code maintainable?
- Should we iterate further or ship?

**3. Update Documentation** üìù

**After all tests pass and human approves**, update relevant documentation:

**Update these locations:**
- `libs/maia-script/docs/developers/` - Developer-facing docs (technical implementation)
- `libs/maia-script/docs/creators/` - Creator-facing docs (how to use features)
- `libs/maia-script/docs/getting-started/` - Getting started guides
- Component-level README files (if applicable)

**DO NOT UPDATE:**
- ‚ùå `libs/maia-script/docs/agents/LLM_*.md` - These are **auto-generated** from developer/creator docs

**To regenerate agent docs after updating:**
```bash
bun run generate:llm-docs
```

**Documentation should include:**
- What changed and why
- New APIs/interfaces/patterns
- Usage examples
- Migration guide (if breaking changes)

**Output**: 
- Fully tested, working solution
- Human-approved implementation
- Updated documentation
- Ready to ship ‚ú®

**LLM Action**: 
1. Run all tests (unit, integration, browser) and verify success
2. Perform comprehensive browser testing:
   - Navigate to app, take screenshots, check console, verify network requests
   - Document browser test results (screenshots, console logs, network summary)
3. Present results (including browser test results) to human for final approval
4. Update relevant documentation (skip LLM docs)
5. Confirm solution is complete

---

## Planning Output Format

When creating a plan, structure it as follows:

```markdown
---
name: [Short descriptive name]
overview: [One sentence summary]
todos:
  - id: milestone-1
    content: [Milestone 1 description]
    status: pending
  - id: milestone-2
    content: [Milestone 2 description]
    status: pending
---

# [Feature/Intent Name]

## Problem Statement
[Clear definition of what we're solving and why]

## Success Criteria
- **Desirable**: [User benefit]
- **Feasible**: [Technical requirement]
- **Viable**: [Maintainability/sustainability requirement]

## Solution Approach
[Chosen approach with rationale]

## Implementation Milestones

### Milestone 1: [Name]
**Tests to Write:**
- [ ] Unit test for X
- [ ] Integration test for Y

**Implementation:**
- [ ] Create file structure
- [ ] Implement core functionality
- [ ] Ensure tests pass

**Cleanup & Migration:**
- [ ] Remove ALL legacy code replaced by this milestone
- [ ] Delete backwards compatibility hacks
- [ ] Update all call sites to new implementation
- [ ] Verify 100% migration complete

**Manual Browser Debugging** (if frontend feature):
- [ ] Open app in Cursor browser
- [ ] Check console for errors and verify real data (no mocks)
- [ ] Check network tab for failed requests
- [ ] Verify basic functionality works
- [ ] Document any issues found

**Human Checkpoint:** ‚úã Pause for feedback

### Milestone 2: [Name]
**Tests to Write:**
- [ ] Unit test for X
- [ ] Integration test for Y

**Implementation:**
- [ ] Integrate with existing systems
- [ ] Implement feature Z
- [ ] Ensure tests pass

**Cleanup & Migration:**
- [ ] Remove ALL legacy code replaced by this milestone
- [ ] Delete backwards compatibility hacks
- [ ] Update all call sites to new implementation
- [ ] Verify 100% migration complete

**Manual Browser Debugging** (if frontend feature):
- [ ] Open app in Cursor browser
- [ ] Check console for errors and verify real data (no mocks)
- [ ] Check network tab for failed requests
- [ ] Verify basic functionality works
- [ ] Document any issues found

**Human Checkpoint:** ‚úã Pause for feedback

### Milestone 3: Documentation & Final Review
- [ ] All tests passing ‚úÖ (unit, integration tests)
- [ ] Manual browser debugging complete (console verified, no errors)
- [ ] Update developer docs
- [ ] Update vibecreator docs
- [ ] Final human approval ‚úã

## File Structure
```
[Detailed file/folder structure with new files highlighted]
```

## Testing Strategy
- **Unit Tests**: [What to test and where]
- **Integration Tests**: [What to test and where]
- **Manual Browser Debugging**: [Quick checks - console errors, network failures, visual bugs]
- **Test Output**: [Expected test summary format]

## Risks & Mitigation
[Potential issues and how to address them]

## Documentation Updates
- [ ] `docs/developers/[file].md`
- [ ] `docs/vibecreators/[file].md`
- [ ] ‚ùå Skip `docs/agents/LLM_*.md` (auto-generated)
```

## Key Mindsets

**Embrace Ambiguity**: Don't expect perfect clarity upfront. Iterate to refine understanding.

**Reframe**: Challenge assumptions. Ask "What if we approached this differently?"

**Make Tangible**: Convert abstract ideas into concrete plans, file structures, and code.

**Iterate**: Plans are living documents. Revisit stages as new information emerges.

**Be Collaborative**: Consider multiple perspectives (users, developers, maintainers).

**Take Action**: Plans are useless without execution. Balance planning with doing.

## When to Use This Process

- ‚úÖ Planning new features or major refactors
- ‚úÖ Introducing new architectural patterns
- ‚úÖ Solving complex, ill-defined problems
- ‚úÖ When requirements are unclear or evolving

**Skip or simplify** for:
- ‚ùå Trivial changes (typos, simple fixes)
- ‚ùå Well-understood, routine tasks
- ‚ùå When the solution is obvious and low-risk

## Process Flexibility

**Non-Linear**: You can:
- Jump between stages (e.g., implement ‚Üí define if tests reveal misunderstanding)
- Run stages in parallel (ideate while still defining)
- Revisit earlier stages based on new insights
- Skip stages if they're not needed for the specific context
- Iterate within implementation: feedback at each milestone may refine approach

**Iterative**: Plans evolve. Start with a rough sketch, refine through stages, iterate based on feedback.

---

## Example Workflow

1. **User**: "I want to add authentication"
2. **Empathize**: "Who needs auth? What auth patterns exist? What are security requirements?"
3. **Define**: "Problem: Users need secure, scalable auth. Success: OAuth2 support, session management, secure by default."
4. **Ideate**: "Options: OAuth2, JWT, session-based. Recommendation: OAuth2 for flexibility."
5. **Prototype**: "Plan: Add auth module, integrate with kernel, create auth middleware, add tests."
6. **Implement**: 
   - Milestone 1: Write auth module tests ‚Üí Implement auth module ‚Üí Clean up old auth code ‚Üí Human feedback ‚úã
   - Milestone 2: Write integration tests ‚Üí Implement kernel integration ‚Üí Remove legacy patterns ‚Üí Human feedback ‚úã
   - Milestone 3: Write middleware tests ‚Üí Implement middleware ‚Üí Delete all backwards compat layers ‚Üí Human feedback ‚úã
7. **Review & Feedback**: "All tests pass ‚úÖ ‚Üí Human approves ‚Üí Update docs ‚Üí Ship! üöÄ"

---

**Remember**: The goal is not a perfect plan, but a **good enough plan** that can be executed, tested, and refined. Start with "good enough" and iterate toward better.
