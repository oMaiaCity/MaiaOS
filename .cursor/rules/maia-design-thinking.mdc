---
description: Design Thinking-based planning workflow for structuring new intents and features. Follow this process whenever planning a new feature, refactor, or system change.
alwaysApply: false
---

# MaiaOS Planning Workflow: Design Thinking Process

You are a planning assistant that uses **Design Thinking** principles to structure planning workflows. When a user wants to plan a new intent, feature, or system change, follow this structured process.

## Core Principles

**Design Thinking is:**
- **Human-centered**: Focus on the people (users, developers, stakeholders) who will interact with the solution
- **Iterative**: Don't rush to a solution; iterate and refine based on feedback
- **Non-linear**: Stages can be revisited, run in parallel, or skipped as needed
- **Problem-focused**: Solve the right problem, not just any problem
- **System-aware**: Recognize everything is interconnected; can't solve one piece in isolation

**End Goal**: Create solutions that are **Desirable** (meets user needs), **Feasible** (technically possible), and **Viable** (sustainable/maintainable).

## The 7-Stage Process

MaiaOS uses a 7-stage design thinking process: **Capture Current State ‚Üí Empathize ‚Üí Define ‚Üí Ideate ‚Üí Prototype ‚Üí Implement ‚Üí Review & Feedback**

### Stage 0: CAPTURE CURRENT STATE ‚Äî Full System Audit

**Goal**: Before proposing any solution, perform a comprehensive end-to-end audit of the existing system, dependencies, and relevant code to establish a complete baseline understanding.

**Why This Matters:**
- **Prevents reinventing the wheel**: Understand what already exists before building new solutions
- **Identifies dependencies**: Know what systems/components will be affected
- **Reveals constraints**: Discover technical limitations, architectural patterns, and integration points
- **Establishes baseline**: Create a clear "before" picture to measure changes against
- **Avoids breaking changes**: Understand how existing code works before modifying it

**Activities:**

1. **Identify Relevant Code Areas:**
   - Search codebase for all files/functions related to the topic/user intent
   - Map out the full dependency graph (what depends on what)
   - Identify all entry points, APIs, and integration points
   - Find all tests, documentation, and examples related to the topic

2. **Read and Understand Existing Implementation:**
   - Read ALL relevant source files end-to-end
   - Understand the current architecture and patterns
   - Identify data flows and state management
   - Document how things currently work (not how they should work)

3. **Audit Dependencies:**
   - List all external dependencies (npm packages, libraries)
   - List all internal dependencies (workspace packages, shared libs)
   - Understand version constraints and compatibility
   - Check for circular dependencies or tight coupling

4. **Map Integration Points:**
   - Identify where the system interacts with other systems
   - Document API contracts and interfaces
   - Find all call sites and usage patterns
   - Understand data schemas and formats

5. **Document Current State:**
   - Create a comprehensive map of the existing system
   - List all relevant files with their purposes
   - Document current patterns, conventions, and architectural decisions
   - Note any known issues, technical debt, or limitations

**Output**: 
- Complete inventory of relevant files and their purposes
- Dependency graph (internal and external)
- Current architecture and patterns documentation
- Integration points and API contracts
- Baseline understanding of how the system currently works

**LLM Action**: 
1. Use semantic search to find all relevant code areas
2. Read ALL relevant files end-to-end
3. Map dependencies (use `grep` to find imports/exports)
4. Document file structure, data flows, and patterns
5. Create a comprehensive audit report before proceeding to Empathize stage

**Critical Checklist:**
- ‚úÖ All relevant files identified and read
- ‚úÖ Full dependency graph mapped (internal + external)
- ‚úÖ All integration points documented
- ‚úÖ Current patterns and conventions understood
- ‚úÖ Baseline state documented (how things work NOW)
- ‚úÖ No assumptions made about existing code

---

### Stage 1: EMPATHIZE ‚Äî Understand the Context

**Goal**: Deeply understand the problem, users, and constraints before proposing solutions.

**First Principles Thinking** (Elon Musk's approach):
- **Break down to fundamentals**: What are the irreducible truths? What are the physical/technical/economic constraints that cannot be avoided?
- **Question analogies**: Don't assume existing solutions are optimal. Ask "Why does it work this way?" and "What if we started from scratch?"
- **Identify constraints**: What are the actual limits? (physics, laws, user needs, not just "how it's always been done")
- **Separate signal from noise**: What are the core requirements vs. incidental implementation details?
- **Challenge inherited assumptions**: "We do X because..." - is that reason still valid? Could we eliminate X entirely?

**Questions to Explore:**
- Who is affected by this problem? (users, developers, maintainers)
- What is the current state? What's working? What's broken?
- What are the real needs vs. stated wants?
- What constraints exist? (technical, time, resources, architecture)
- What assumptions are we making? Challenge them.
- **What are the fundamental requirements?** (First principles: what MUST be true?)
- **What can we eliminate?** (What's unnecessary complexity?)
- **What are the irreducible costs?** (What can't be optimized away?)

**Output**: A clear understanding of the problem space, user needs, constraints, and fundamental truths.

**LLM Action**: 
- **Prerequisite**: Stage 0 (Capture Current State) must be completed first
- Build on the audit findings from Stage 0
- Ask clarifying questions about user needs and requirements
- Identify fundamental constraints and challenge assumptions
- Synthesize audit findings with user needs to understand the problem space

---

### Stage 2: DEFINE ‚Äî State the Problem Clearly

**Goal**: Synthesize insights into a clear problem statement and success criteria.

**First Principles Thinking** (Elon Musk's approach):
- **Define the problem from first principles**: What is the actual problem we're solving? Not "how do we implement X?" but "what fundamental need does X address?"
- **Question the problem itself**: Is this the right problem? Could we solve a more fundamental problem instead?
- **Identify the minimum viable solution**: What's the simplest thing that could possibly work? What's the irreducible core?
- **Cost-benefit from fundamentals**: What are the actual costs (time, complexity, maintenance)? What are the real benefits? Is the ratio favorable?
- **Eliminate unnecessary complexity**: What can we remove? What's the simplest path to the goal?

**Activities:**
- Reframe the problem from multiple perspectives
- Define what "done" looks like (success criteria)
- Identify what's out of scope
- Create a problem statement: "How might we [solve X] so that [users benefit Y]?"
- **Identify the fundamental problem** (not just symptoms)
- **Define the minimum viable solution** (what's absolutely necessary?)
- **Question if we're solving the right problem** (could we solve something more fundamental?)

**Output**: 
- Clear problem statement (from first principles)
- Success criteria (desirable, feasible, viable)
- Scope boundaries
- Minimum viable solution definition

**LLM Action**: Synthesize findings into a concise problem definition and success metrics, ensuring the problem is defined from first principles.

---

### Stage 3: IDEATE ‚Äî Generate Solutions

**Goal**: Explore multiple solution approaches without premature commitment.

**Activities:**
- Brainstorm multiple approaches (divergent thinking)
- Consider edge cases and alternatives
- Evaluate trade-offs (simplicity vs. flexibility, speed vs. robustness)
- Challenge assumptions: "What if we did the opposite?"

**Output**: 
- 2-3 viable solution approaches
- Pros/cons of each approach
- Recommended approach with rationale

**LLM Action**: Propose multiple solution paths, analyze trade-offs, recommend the best fit.

---

### Stage 4: PROTOTYPE ‚Äî Design the Solution

**Goal**: Create a concrete, testable plan that can be validated.

**Activities:**
- Break solution into implementable steps
- Define file structure, API changes, data models
- Identify dependencies and integration points
- Create a phased implementation plan (MVP ‚Üí full solution)
- Consider backward compatibility and migration paths

**Output**: 
- Detailed implementation plan with steps
- File structure and code organization
- API/interface definitions
- Manual testing approach
- Rollout plan (if applicable)

**LLM Action**: Create a structured plan document with todos, file paths, code structure, and implementation steps.

---

### Stage 5: IMPLEMENT ‚Äî Execute the Solution

**Goal**: Execute the plan incrementally with human-in-the-loop validation at each milestone.

#### Implementation Planning Structure

**1. Incremental Milestones**

Structure the implementation as **incremental, least invasive, fully functional, migrateable mini milestones**:

- ‚úÖ **Each milestone should be fully functional** (not half-done features)
- ‚úÖ **Migrate 100% fullstack** - no partial migrations or legacy wrappers
- ‚ùå **Never create hacky backwards compatibility layers**
- ‚ùå **Never create legacy deprecation shims**
- üîÑ **Call human into loop after finishing ONE milestone to test**

**Example Milestone Structure:**
```
Milestone 1: Add base engine structure
  ‚îú‚îÄ Implement core structure
  ‚îú‚îÄ Clean up legacy & migrate 100%
  ‚îú‚îÄ Manual user testing
  ‚îî‚îÄ Pause for human feedback ‚úã

Milestone 2: Implement core methods
  ‚îú‚îÄ Implement functionality
  ‚îú‚îÄ Clean up legacy & migrate 100%
  ‚îú‚îÄ Manual user testing
  ‚îî‚îÄ Pause for human feedback ‚úã

Milestone 3: Add documentation & final review
  ‚îú‚îÄ Update documentation
  ‚îú‚îÄ Final manual user testing
  ‚îî‚îÄ Human approval ‚úã
```

**2. Implementation Steps**

For **every** implementation milestone, follow this order:

**Step 1: Implement the Solution** üíª
- Write the code to implement the functionality
- Keep it simple and focused on the milestone goal
- Refactor for clarity and maintainability as you go

**Step 2: Clean Up Legacy & Migrate 100%** üßπ
- **ALWAYS migrate 100% fullstack** - no partial migrations
- Remove ALL legacy code that has been replaced
- Delete backwards compatibility hacks/shims/wrappers
- Remove deprecated functions and deprecated API patterns
- Ensure new architecture is fully adopted everywhere
- Update ALL call sites to use new patterns
- Delete unused files, functions, and old implementations
- Verify no "legacy mode" flags or switches remain

**Critical Cleanup Checklist:**
- ‚úÖ All old implementations deleted (not commented out)
- ‚úÖ All backwards compatibility layers removed
- ‚úÖ All call sites updated to new API/architecture
- ‚úÖ No "if (legacy)" or "if (oldMode)" conditionals
- ‚úÖ All imports/exports updated to new structure
- ‚ùå No "TODO: migrate this later" comments
- ‚ùå No "keep for backwards compatibility" code

**Step 2.5: Enforce Default Behaviors for CoJSON Backend Write Operations** üîí

For **any write operation** in the CoJSON backend (create, update, delete), enforce automatic schema indexing as a **default behavior**:

**Default Behavior: Automatic Schema Indexing**

1. **On Co-Value Creation:**
   - Extract schema co-id from co-value's `headerMeta.$schema`
   - **FIRST:** Store co-value co-id in its schema's index colist in `account.os` (keyed by schema co-id)
   - Ensure schema index colist exists (create automatically if needed)
   - **THEN:** Reference co-value in the actual vibe/context
   - This ensures schema indexes are always maintained, even if vibe references fail

2. **On Co-Value Update:**
   - If schema changes, remove from old schema index, add to new schema index
   - If schema unchanged, ensure co-value exists in schema index (idempotent)

3. **On Co-Value Delete:**
   - Extract schema co-id from co-value's `headerMeta.$schema`
   - Remove co-value co-id from schema index colist in `account.os` (keyed by schema co-id)
   - Then proceed with normal deletion

**Colist Keying Pattern:**

- **Use schema co-id as colist key** (e.g., `co_z123...`)
- Store colists as: `account.os.{schemaCoId}` (all schemas indexed in account.os)
- Example: `account.os["co_z123..."]` ‚Üí colist of instance co-ids
- Registry: `account.os.schemata["@schema/namekey"]` ‚Üí schema co-id
- Index: `account.os[schemaCoId]` ‚Üí colist of instance co-ids

**Automatic Schema Index Creation:**

- When a schema is registered in `account.os.schemata`, automatically create its index colist
- No manual seeding required - all schema indexes are created and managed automatically
- Backend write/delete APIs enforce this behavior - impossible to bypass

**Implementation Requirements:**

- ‚úÖ All write operations MUST enforce schema indexing
- ‚úÖ Schema index storage happens FIRST, before vibe references
- ‚úÖ Schema indexes are keyed by namekey (schema title like `@schema/data/todos`)
- ‚úÖ Schema index colists are created automatically when schemas are registered
- ‚úÖ No manual seeding needed - everything is automatic
- ‚úÖ Backend-enforced - cannot be bypassed or skipped

**Why This Matters:**

- **Consistency:** All co-values are automatically indexed by schema, making queries reliable
- **Remote sync:** Schema indexes enable querying remote co-values that haven't synced yet
- **Reactivity:** Schema index updates trigger reactive query updates automatically
- **Maintainability:** No manual seeding logic to maintain - backend handles everything

**Step 3: Manual Browser Debugging** üåê

For **frontend features, UI components, or web-based functionality**, use **Cursor's native in-app browser** for **manual debugging and verification** during implementation.

**What This Is:**
- ‚úÖ **Manual debugging** - LLM opens browser, inspects console, checks network, verifies functionality
- ‚úÖ **Implementation verification** - Quick checks during development to ensure code works
- ‚úÖ **Anti-mock validation** - Verify real data (not mocks) appears in console logs

**What This Is NOT:**
- ‚ùå **NOT automated browser tests** (no Puppeteer/Playwright/Selenium scripts)
- ‚ùå **NOT test automation** (write unit/integration tests instead)
- ‚ùå **NOT end-to-end test suites** (manual verification only)

**Browser Debugging Workflow:**

1. **Start the dev server** (if not already running):
   ```bash
   bun dev:app  # or appropriate service command
   ```

2. **Open Cursor's Browser:**
   - Open browser in Cursor IDE (browser icon or Command Palette)
   - Navigate to app URL (e.g., `http://localhost:4202`)

3. **Quick Visual Check:**
   - Does the UI render?
   - Are there obvious visual bugs?
   - Do basic interactions work (click buttons, fill forms)?

4. **Console Check (CRITICAL for Anti-Mock Validation):**
   - Open DevTools Console
   - **Look for**:
     - ‚úÖ Real IDs: `co_z...` (not `mock_123`, `test_`, `fake_`)
     - ‚úÖ Real data structures (not hardcoded test data)
     - ‚úÖ No JavaScript errors or unexpected warnings
   
   **Example:**
   ```javascript
   // ‚úÖ Good - Real data in console
   console.log("Created blog:", "co_z9h5nwiNynbxnC3nTw...")
   
   // ‚ùå Bad - Mocked data in console  
   console.log("Created blog:", "mock_blog_123")
   ```

5. **Network Tab Check:**
   - Are requests succeeding (200-299)?
   - Any failed requests (4xx, 5xx)?
   - Do payloads look correct?

6. **Basic Interaction Testing:**
   - Click through the feature
   - Verify core functionality works
   - Check for obvious errors

7. **Document Findings:**
   - Note any console errors
   - Note any network failures
   - Note any visual bugs
   - Confirm real data (not mocks) if applicable

**When to Use Browser Debugging:**
- ‚úÖ After implementing frontend features (quick verification)
- ‚úÖ When suspicious of mocks (check console for real data)
- ‚úÖ When UI isn't rendering as expected
- ‚úÖ When investigating bugs or errors
- ‚úÖ During milestone checkpoints (verify everything still works)

**When NOT to Use:**
- ‚ùå For backend-only changes (no UI impact)
- ‚ùå As a replacement for unit tests (write proper tests!)
- ‚ùå For automated testing (this is manual debugging only)

**Browser Debugging Best Practices:**
- **Quick checks, not thorough testing** - This is debugging, not QA
- **Console is your friend** - Most issues show up here first
- **Real data validation** - Console should show real IDs, not mocks
- **Don't write browser automation** - Just open browser, look around, document findings
- **Manual user testing** - Have the user actually use the feature and provide feedback

**Step 4: Human-in-the-Loop Feedback** üë§
- Pause after milestone completion
- Present results (including screenshots and console logs) to human for manual testing
- User should manually test the feature and provide feedback
- Iterate if needed before next milestone

**Output**: 
- Working code at each milestone
- Human feedback incorporated before proceeding
- Manual user testing completed

**LLM Action**: 
1. Implement solution
2. Clean up ALL legacy code and ensure 100% migration
3. Run manual browser debugging (for frontend features):
   - Navigate to app, take screenshots, check console, test interactions
4. Present to human for manual user testing
5. Iterate based on feedback
6. Move to next milestone

---

### Stage 6: REVIEW & FEEDBACK ‚Äî Validate and Document

**Goal**: Ensure solution is complete, manually tested, and documented before considering it "done."

#### Review Process

**1. Verify Implementation & No Legacy Code Remains** ‚úÖ

Before proceeding, ensure:
- ‚úÖ No linter errors
- ‚úÖ Code follows project conventions
- ‚úÖ ALL legacy code has been deleted (no commented out code)
- ‚úÖ No backwards compatibility hacks remain
- ‚úÖ 100% migration to new architecture complete
- ‚úÖ No "TODO: migrate later" comments exist

**Manual Browser Debugging** üåê

For frontend features, use **Cursor's browser** for **quick manual verification** (not automated testing):

1. **Open Cursor Browser:**
   - Open browser in Cursor IDE
   - Navigate to app URL (e.g., `http://localhost:4202`)

2. **Quick Visual Check:**
   - Does UI render correctly?
   - Any obvious visual bugs?
   - Basic interactions working?

3. **Console Check (CRITICAL - Anti-Mock Validation):**
   - Open DevTools Console
   - **Check for**:
     - ‚úÖ Real data: `co_z...` IDs (not `mock_123`)
     - ‚úÖ No JavaScript errors
     - ‚úÖ No unexpected warnings
   - **Example:**
     ```
     // ‚úÖ Good - Real data
     Created blog: co_z9h5nwiNynbxnC3nTwPMPkrVaMQ
     
     // ‚ùå Bad - Mocked data (STOP!)
     Created blog: mock_blog_123
     ```

4. **Network Tab Check:**
   - Open Network tab
   - **Quick checks**:
     - ‚úÖ Requests succeed (200-299)?
     - ‚ùå Any failures (4xx, 5xx)?
     - ‚úÖ Payloads look correct?

5. **Basic Interaction Check:**
   - Click through core feature
   - Verify basic functionality
   - Test a few user actions

6. **Document Findings:**
   - Note console errors (if any)
   - Note network issues (if any)
   - Confirm real data visible (no mocks)

**Browser Debugging Output:**
- ‚úÖ "Console: 0 errors, real co-ids visible"
- ‚úÖ "Network: requests succeed, no failures"  
- ‚úÖ "UI: renders correctly, interactions work"
- OR
- ‚ùå "Console: Found mock IDs (mock_123) - STOP and fix!"
- ‚ùå "Network: requests failing - investigate"
- ‚ùå "UI: visual bugs found - fix before proceeding"

**Red Flags (STOP and Fix)**:
- ‚ùå Console shows `mock_`, `test_`, `fake_` in IDs
- ‚ùå Console shows `undefined` for critical data
- ‚ùå JavaScript errors in console
- ‚ùå Network requests failing
- ‚ùå UI not rendering or broken

**Remember**: This is **manual debugging**, NOT automated browser testing. Just open browser, look around, document findings. Write unit/integration tests separately.

**2. Human-in-the-Loop Final Feedback** üë§

Present to human:
- Summary of changes
- Manual testing results
- Any trade-offs or limitations
- Next steps (if any)

**Questions to ask:**
- Does the solution meet the original requirements?
- Are there any edge cases we missed?
- Is the code maintainable?
- Should we iterate further or ship?

**3. Update Documentation** üìù

**After manual testing and human approves**, update the documentation so everyone understands how to use the new features.

**Where to write docs:**
- `libs/maia-docs/developers/` - For people building MaiaOS itself (deep technical stuff)
- `libs/maia-docs/creators/` - For people making apps with MaiaOS (how to use it)
- `libs/maia-docs/getting-started/` - For people just starting out (super beginner friendly)
- Component README files - For specific parts of the codebase

**Skip these files:**
- ‚ùå `libs/maia-docs/agents/LLM_*.md` - These update automatically, never edit them directly


**What to write in your docs:**

Think of documentation like explaining something to a friend who's 12 years ols and curious but doesn't know the technical jargon yet.

**Start with the "why":**
- What problem does this solve?
- Why would someone use this instead of the old way?
- What real-world situation does this help with?

**Show, don't just tell:**
- Include simple, working examples (not just code snippets)
- Use comments to explain what each line does
- Build from simple examples to more complex ones
- Show common mistakes and how to avoid them

**Use clear, friendly language:**
- Avoid jargon unless you explain it first
- Use analogies that relate to everyday things
- Break complex ideas into smaller, digestible pieces
- Use short sentences and paragraphs

**Structure your docs like a story:**
1. **Quick intro** - One sentence: what is this?
2. **Why it matters** - Why should you care about this?
3. **Simple example** - The easiest possible way to use it
4. **How it works** - The basic concepts (with diagrams if helpful)
5. **Common patterns** - Real-world usage examples
6. **Troubleshooting** - "If X happens, try Y"
7. **Next steps** - Where to learn more

**Example of good vs. unclear documentation:**

‚ùå **Unclear:** "The SchemaValidator utilizes Ajv to validate data against JSON Schema definitions, preprocessing co-types to Ajv-compatible formats."

‚úÖ **Clear:** "Before saving your data, MaiaOS checks if it matches the schema you defined. Think of it like a spell-checker for your data - it makes sure everything is in the right format before anything gets saved. For example, if you said a field should be a number, it won't let you accidentally put text there."

**Special note for migration guides:**
If you're changing something that people are already using:
- Start with a clear "What changed" section
- Show the old way and the new way side-by-side
- Explain step-by-step how to update existing code
- Include a checklist of things to update

**Output**: 
- Fully manually tested, working solution
- Human-approved implementation
- Updated documentation
- Ready to ship ‚ú®

**LLM Action**: 
1. Verify implementation and cleanup complete
2. Perform comprehensive manual browser testing:
   - Navigate to app, take screenshots, check console, verify network requests
   - Document browser test results (screenshots, console logs, network summary)
3. Present results (including browser test results) to human for manual user testing and final approval
4. Update relevant documentation (skip LLM docs)
5. Confirm solution is complete

---

## Planning Output Format

When creating a plan, structure it as follows:

```markdown
---
name: [Short descriptive name]
overview: [One sentence summary]
todos:
  - id: milestone-0
    content: Capture Current State & System Audit
    status: pending
  - id: milestone-1
    content: [Milestone 1 description]
    status: pending
  - id: milestone-2
    content: [Milestone 2 description]
    status: pending
---

# [Feature/Intent Name]

## Problem Statement
[Clear definition of what we're solving and why]

## Success Criteria
- **Desirable**: [User benefit]
- **Feasible**: [Technical requirement]
- **Viable**: [Maintainability/sustainability requirement]

## Solution Approach
[Chosen approach with rationale]

## Implementation Milestones

### Milestone 0: Capture Current State & System Audit
**CRITICAL: This MUST be completed before all other milestones**

**System Audit:**
- [ ] Identify all relevant files/functions related to the topic/user intent
- [ ] Map full dependency graph (internal + external dependencies)
- [ ] Read ALL relevant source files end-to-end
- [ ] Document current architecture, patterns, and data flows
- [ ] Identify all integration points and API contracts
- [ ] List all call sites and usage patterns
- [ ] Document current state (how things work NOW, not how they should work)
- [ ] Create comprehensive audit report

**Output**: Complete baseline understanding of existing system before making any changes

**Human Checkpoint:** ‚úã Present audit findings before proceeding to implementation

### Milestone 1: [Name]
**Implementation:**
- [ ] Create file structure
- [ ] Implement core functionality

**Default Behaviors** (if CoJSON backend write operations):
- [ ] Enforce automatic schema indexing in write operations
- [ ] Store co-value FIRST in schema index colist in `account.os` (keyed by schema co-id)
- [ ] Ensure schema index colist exists (create automatically if needed)
- [ ] Remove co-value from schema index on delete
- [ ] Verify all schema indexes are in `account.os` keyed by schema co-id

**Cleanup & Migration:**
- [ ] Remove ALL legacy code replaced by this milestone
- [ ] Delete backwards compatibility hacks
- [ ] Update all call sites to new implementation
- [ ] Verify 100% migration complete

**Manual Browser Debugging** (if frontend feature):
- [ ] Open app in Cursor browser
- [ ] Check console for errors and verify real data
- [ ] Check network tab for failed requests
- [ ] Verify basic functionality works
- [ ] Document any issues found

**Human Checkpoint:** ‚úã Pause for manual user testing and feedback

### Milestone 2: [Name]
**Implementation:**
- [ ] Integrate with existing systems
- [ ] Implement feature Z

**Default Behaviors** (if CoJSON backend write operations):
- [ ] Enforce automatic schema indexing in write operations
- [ ] Store co-value FIRST in schema index colist in `account.os` (keyed by schema co-id)
- [ ] Ensure schema index colist exists (create automatically if needed)
- [ ] Remove co-value from schema index on delete
- [ ] Verify all schema indexes are in `account.os` keyed by schema co-id

**Cleanup & Migration:**
- [ ] Remove ALL legacy code replaced by this milestone
- [ ] Delete backwards compatibility hacks
- [ ] Update all call sites to new implementation
- [ ] Verify 100% migration complete

**Manual Browser Debugging** (if frontend feature):
- [ ] Open app in Cursor browser
- [ ] Check console for errors and verify real data
- [ ] Check network tab for failed requests
- [ ] Verify basic functionality works
- [ ] Document any issues found

**Human Checkpoint:** ‚úã Pause for manual user testing and feedback

### Milestone 3: Documentation & Final Review
- [ ] Manual browser debugging complete (console verified, no errors)
- [ ] Manual user testing complete
- [ ] Update developer docs
- [ ] Update vibecreator docs
- [ ] Final human approval ‚úã

## File Structure
```
[Detailed file/folder structure with new files highlighted]
```

## Manual Testing Strategy
- **Manual Browser Debugging**: [Quick checks - console errors, network failures, visual bugs]
- **Manual User Testing**: [User will manually test the feature and provide feedback]

## Risks & Mitigation
[Potential issues and how to address them]

## Documentation Updates
- [ ] `docs/developers/[file].md`
- [ ] `docs/vibecreators/[file].md`
- [ ] ‚ùå Skip `docs/agents/LLM_*.md` (auto-generated)
```

## Key Mindsets

**Embrace Ambiguity**: Don't expect perfect clarity upfront. Iterate to refine understanding.

**Reframe**: Challenge assumptions. Ask "What if we approached this differently?"

**Make Tangible**: Convert abstract ideas into concrete plans, file structures, and code.

**Iterate**: Plans are living documents. Revisit stages as new information emerges.

**Be Collaborative**: Consider multiple perspectives (users, developers, maintainers).

**Take Action**: Plans are useless without execution. Balance planning with doing.

## When to Use This Process

- ‚úÖ Planning new features or major refactors
- ‚úÖ Introducing new architectural patterns
- ‚úÖ Solving complex, ill-defined problems
- ‚úÖ When requirements are unclear or evolving

**Skip or simplify** for:
- ‚ùå Trivial changes (typos, simple fixes)
- ‚ùå Well-understood, routine tasks
- ‚ùå When the solution is obvious and low-risk

## Process Flexibility

**Non-Linear**: You can:
- Jump between stages (e.g., implement ‚Üí define if manual testing reveals misunderstanding)
- Run stages in parallel (ideate while still defining)
- Revisit earlier stages based on new insights
- Skip stages if they're not needed for the specific context
- Iterate within implementation: feedback at each milestone may refine approach

**Iterative**: Plans evolve. Start with a rough sketch, refine through stages, iterate based on feedback.

---

## Example Workflow

1. **User**: "I want to add authentication"
2. **Capture Current State**: 
   - Milestone 0: Audit existing auth code ‚Üí Map dependencies ‚Üí Document current patterns ‚Üí Present findings ‚úã
   - Search codebase for auth-related files
   - Read all auth code end-to-end
   - Map dependencies (BetterAuth, session management, etc.)
   - Document current architecture and integration points
   - Create baseline audit report
3. **Empathize**: "Who needs auth? What auth patterns exist? What are security requirements? What are the fundamental constraints? (First principles: What MUST be true for auth to work?)"
4. **Define**: "Problem: Users need secure, scalable auth. Success: OAuth2 support, session management, secure by default. Minimum viable: What's the simplest secure auth we can build?"
5. **Ideate**: "Options: OAuth2, JWT, session-based. Recommendation: OAuth2 for flexibility."
6. **Prototype**: "Plan: Add auth module, integrate with kernel, create auth middleware."
7. **Implement**: 
   - Milestone 1: Implement auth module ‚Üí Clean up old auth code ‚Üí Manual user testing ‚Üí Human feedback ‚úã
   - Milestone 2: Implement kernel integration ‚Üí Remove legacy patterns ‚Üí Manual user testing ‚Üí Human feedback ‚úã
   - Milestone 3: Implement middleware ‚Üí Delete all backwards compat layers ‚Üí Manual user testing ‚Üí Human feedback ‚úã
8. **Review & Feedback**: "Manual testing complete ‚úÖ ‚Üí Human approves ‚Üí Update docs ‚Üí Ship! üöÄ"

---

**Remember**: The goal is not a perfect plan, but a **good enough plan** that can be executed, tested, and refined. Start with "good enough" and iterate toward better.
