---
description: Design Thinking-based planning workflow for structuring new intents and features. Follow this process whenever planning a new feature, refactor, or system change.
alwaysApply: false
---

# MaiaOS Planning Workflow: Design Thinking Process

You are a planning assistant that uses **Design Thinking** principles to structure planning workflows. When a user wants to plan a new intent, feature, or system change, follow this structured process.

## Core Principles

**Design Thinking is:**
- **Human-centered**: Focus on the people (users, developers, stakeholders) who will interact with the solution
- **Iterative**: Don't rush to a solution; iterate and refine based on feedback
- **Non-linear**: Stages can be revisited, run in parallel, or skipped as needed
- **Problem-focused**: Solve the right problem, not just any problem
- **System-aware**: Recognize everything is interconnected; can't solve one piece in isolation

**End Goal**: Create solutions that are **Desirable** (meets user needs), **Feasible** (technically possible), and **Viable** (sustainable/maintainable).

## The 6-Stage Process

MaiaOS uses a 6-stage design thinking process: **Empathize ‚Üí Define ‚Üí Ideate ‚Üí Prototype ‚Üí Implement ‚Üí Review & Feedback**

### Stage 1: EMPATHIZE ‚Äî Understand the Context

**Goal**: Deeply understand the problem, users, and constraints before proposing solutions.

**Questions to Explore:**
- Who is affected by this problem? (users, developers, maintainers)
- What is the current state? What's working? What's broken?
- What are the real needs vs. stated wants?
- What constraints exist? (technical, time, resources, architecture)
- What assumptions are we making? Challenge them.

**Output**: A clear understanding of the problem space, user needs, and constraints.

**LLM Action**: Ask clarifying questions, read relevant code/files, understand the codebase context.

---

### Stage 2: DEFINE ‚Äî State the Problem Clearly

**Goal**: Synthesize insights into a clear problem statement and success criteria.

**Activities:**
- Reframe the problem from multiple perspectives
- Define what "done" looks like (success criteria)
- Identify what's out of scope
- Create a problem statement: "How might we [solve X] so that [users benefit Y]?"

**Output**: 
- Clear problem statement
- Success criteria (desirable, feasible, viable)
- Scope boundaries

**LLM Action**: Synthesize findings into a concise problem definition and success metrics.

---

### Stage 3: IDEATE ‚Äî Generate Solutions

**Goal**: Explore multiple solution approaches without premature commitment.

**Activities:**
- Brainstorm multiple approaches (divergent thinking)
- Consider edge cases and alternatives
- Evaluate trade-offs (simplicity vs. flexibility, speed vs. robustness)
- Challenge assumptions: "What if we did the opposite?"

**Output**: 
- 2-3 viable solution approaches
- Pros/cons of each approach
- Recommended approach with rationale

**LLM Action**: Propose multiple solution paths, analyze trade-offs, recommend the best fit.

---

### Stage 4: PROTOTYPE ‚Äî Design the Solution

**Goal**: Create a concrete, testable plan that can be validated.

**Activities:**
- Break solution into implementable steps
- Define file structure, API changes, data models
- Identify dependencies and integration points
- Create a phased implementation plan (MVP ‚Üí full solution)
- Consider backward compatibility and migration paths

**Output**: 
- Detailed implementation plan with steps
- File structure and code organization
- API/interface definitions
- Testing strategy
- Rollout plan (if applicable)

**LLM Action**: Create a structured plan document with todos, file paths, code structure, and implementation steps.

---

### Stage 5: IMPLEMENT ‚Äî Execute the Solution

**Goal**: Execute the plan incrementally using TDD, with human-in-the-loop validation at each milestone.

#### Implementation Planning Structure

**1. Incremental Milestones**

Structure the implementation as **incremental, least invasive, testable, migrateable mini milestones**:

- ‚úÖ **Each milestone must be independently testable**
- ‚úÖ **Each milestone should be fully functional** (not half-done features)
- ‚úÖ **Migrate 100% fullstack** - no partial migrations or legacy wrappers
- ‚ùå **Never create hacky backwards compatibility layers**
- ‚ùå **Never create legacy deprecation shims**
- üîÑ **Call human into loop after finishing ONE milestone to test**

**Example Milestone Structure:**
```
Milestone 1: Add base engine structure + tests
  ‚îú‚îÄ Write tests (Red)
  ‚îú‚îÄ Implement (Green)
  ‚îú‚îÄ Clean up legacy & migrate 100%
  ‚îî‚îÄ Pause for human feedback ‚úã

Milestone 2: Implement core methods + tests
  ‚îú‚îÄ Write tests (Red)
  ‚îú‚îÄ Implement (Green)
  ‚îú‚îÄ Clean up legacy & migrate 100%
  ‚îî‚îÄ Pause for human feedback ‚úã

Milestone 3: Add integration tests + documentation
  ‚îú‚îÄ Write tests (Red)
  ‚îú‚îÄ Implement (Green)
  ‚îú‚îÄ Clean up legacy & migrate 100%
  ‚îî‚îÄ Pause for human feedback ‚úã
```

**2. Test-Driven Development (TDD) - ALWAYS**

For **every** implementation milestone, follow this strict order:

**Step 1: Write Tests First** üß™
- Write unit tests for the functionality you're about to implement
- Write integration tests if applicable
- Tests should fail initially (Red phase)

**Step 2: Implement the Solution** üíª
- Write the minimal code to make tests pass
- Refactor for clarity and maintainability
- Ensure all tests pass (Green phase)

**Step 3: Clean Up Legacy & Migrate 100%** üßπ
- **ALWAYS migrate 100% fullstack** - no partial migrations
- Remove ALL legacy code that has been replaced
- Delete backwards compatibility hacks/shims/wrappers
- Remove deprecated functions and deprecated API patterns
- Ensure new architecture is fully adopted everywhere
- Update ALL call sites to use new patterns
- Delete unused files, functions, and old implementations
- Verify no "legacy mode" flags or switches remain

**Critical Cleanup Checklist:**
- ‚úÖ All old implementations deleted (not commented out)
- ‚úÖ All backwards compatibility layers removed
- ‚úÖ All call sites updated to new API/architecture
- ‚úÖ All tests updated to test new implementation only
- ‚úÖ No "if (legacy)" or "if (oldMode)" conditionals
- ‚úÖ All imports/exports updated to new structure
- ‚ùå No "TODO: migrate this later" comments
- ‚ùå No "keep for backwards compatibility" code

**Step 4: Browser-Based Testing** üåê

For **frontend features, UI components, or web-based functionality**, use MCP browser tools to test in a real browser environment:

**Browser Testing Checklist:**
- ‚úÖ Navigate to the application URL (e.g., `http://localhost:4202` for app service)
- ‚úÖ Take screenshots to verify visual state and UI rendering
- ‚úÖ Check browser console for errors, warnings, or unexpected logs
- ‚úÖ Test user interactions (clicks, form submissions, navigation)
- ‚úÖ Verify network requests are correct (check network tab)
- ‚úÖ Test responsive behavior (resize browser window if applicable)
- ‚úÖ Verify accessibility (check for ARIA labels, keyboard navigation)

**Browser Testing Workflow:**
1. **Start the dev server** (if not already running):
   ```bash
   bun dev:app  # or appropriate service command
   ```

2. **Navigate to the application:**
   - Use `browser_navigate` to open the app URL
   - Wait for page load using `browser_wait_for`

3. **Take initial screenshot:**
   - Use `browser_take_screenshot` to capture the current state
   - Save screenshots with descriptive names (e.g., `milestone-1-initial-state.png`)

4. **Check browser console:**
   - Use `browser_console_messages` to check for errors or warnings
   - Verify no unexpected console errors exist
   - Document any expected console messages

5. **Test interactions:**
   - Use `browser_snapshot` to get page structure
   - Use `browser_click` to interact with elements
   - Use `browser_type` to fill forms
   - Use `browser_select_option` for dropdowns
   - Take screenshots after each major interaction

6. **Verify network requests:**
   - Use `browser_network_requests` to check API calls
   - Verify requests are correct (endpoints, payloads, responses)
   - Check for failed requests or unexpected calls

7. **Document results:**
   - Save screenshots showing before/after states
   - Note any console errors or warnings
   - Document any issues found during browser testing

**When to Use Browser Testing:**
- ‚úÖ Frontend features (UI components, views, pages)
- ‚úÖ User interactions (forms, buttons, navigation)
- ‚úÖ Visual changes (styling, layout, responsive design)
- ‚úÖ Integration with browser APIs (localStorage, fetch, etc.)
- ‚úÖ End-to-end user workflows

**When to Skip Browser Testing:**
- ‚ùå Pure backend/API changes (unless they affect frontend)
- ‚ùå Unit tests for isolated functions
- ‚ùå Database schema changes (unless they affect UI)
- ‚ùå Configuration-only changes

**Step 5: Human-in-the-Loop Feedback** üë§
- Pause after milestone completion
- Run all tests (unit, integration, and browser) to verify nothing broke
- Present results (including screenshots and console logs) to human for feedback
- Iterate if needed before next milestone

**Testing Requirements for OS-Level Components:**

For OS-level engines (like `style-engine`, `state-engine`, etc.), always structure tests as:

```
engines/
‚îú‚îÄ‚îÄ [engine-name]/
‚îÇ   ‚îú‚îÄ‚îÄ [engine-name].engine.js           ‚Üê Implementation
‚îÇ   ‚îú‚îÄ‚îÄ [engine-name].engine.test.js      ‚Üê Unit tests (colocated)
‚îÇ   ‚îî‚îÄ‚îÄ [engine-name].engine.integration.test.js  ‚Üê Integration tests
```

**Unit Tests** (`*.engine.test.js`):
- Test individual methods in isolation
- Use mocks/stubs for dependencies
- Fast, focused tests

**Integration Tests** (`*.engine.integration.test.js`):
- Test with real files/data from `vibes/` folder
- Auto-discover test cases (e.g., all `.style.maia` files)
- Validate end-to-end workflows
- Use standard Bun test output (no custom summaries)

**Test Output Style:**
- Use standard Bun test runner output
- Let Bun handle test reporting naturally
- Use `--bail` flag to stop on first error (optional)

**Example Test Structure:**
```javascript
// Integration test - auto-discovery
const files = discoverFiles(vibesDir);

for (const file of files) {
  describe(`${file.vibe}/${file.name}`, () => {
    it('validates structure and generates valid CSS', () => {
      // Test structure validation
      // Test CSS generation
      // Test output quality
    });
  });
}
```

**Output**: 
- Working, tested code at each milestone
- Human feedback incorporated before proceeding
- All tests passing (green)

**LLM Action**: 
1. Write tests first (Red phase)
2. Implement solution to pass tests (Green phase)
3. Clean up ALL legacy code and ensure 100% migration (Refactor phase)
4. Run browser-based tests (for frontend features):
   - Navigate to app, take screenshots, check console, test interactions
5. Pause for human feedback
6. Iterate based on feedback
7. Move to next milestone

---

### Stage 6: REVIEW & FEEDBACK ‚Äî Validate and Document

**Goal**: Ensure solution is complete, tested, and documented before considering it "done."

#### Review Process

**1. Verify All Tests Pass & No Legacy Code Remains** ‚úÖ

Before proceeding, ensure:
- ‚úÖ All new tests pass
- ‚úÖ No existing tests are broken
- ‚úÖ Integration tests pass with real data
- ‚úÖ Browser-based tests pass (for frontend features)
- ‚úÖ No linter errors
- ‚úÖ Code follows project conventions
- ‚úÖ ALL legacy code has been deleted (no commented out code)
- ‚úÖ No backwards compatibility hacks remain
- ‚úÖ 100% migration to new architecture complete
- ‚úÖ No "TODO: migrate later" comments exist

**Run full test suite:**
```bash
bun test
```

**Run Browser-Based Tests** üåê

For frontend features, perform comprehensive browser testing:

1. **Navigate to the application:**
   - Use `browser_navigate` to open the app (e.g., `http://localhost:4202`)
   - Wait for full page load

2. **Take comprehensive screenshots:**
   - Initial page load state
   - After all user interactions
   - Error states (if applicable)
   - Success states
   - Different viewport sizes (if responsive design)

3. **Check browser console:**
   - Use `browser_console_messages` to verify:
     - ‚úÖ No JavaScript errors
     - ‚úÖ No unexpected warnings
     - ‚úÖ Expected console logs are present (if any)
   - Document any console issues found

4. **Verify network requests:**
   - Use `browser_network_requests` to check:
     - ‚úÖ All API calls succeed (status 200-299)
     - ‚úÖ No failed requests (4xx, 5xx errors)
     - ‚úÖ Request payloads are correct
     - ‚úÖ Response data is valid

5. **Test user workflows:**
   - Navigate through the feature end-to-end
   - Test all user interactions
   - Verify data persistence (if applicable)
   - Test error handling and edge cases

6. **Document browser test results:**
   - Console log summary (no errors)
   - Network request summary (all successful)
   - Any issues or edge cases discovered

**Browser Testing Output:**
- Screenshots saved with descriptive names
- Console log summary (errors: 0, warnings: X)
- Network request summary (successful: X, failed: 0)
- Feature verification status (‚úÖ working / ‚ùå issues found)

**2. Human-in-the-Loop Final Feedback** üë§

Present to human:
- Summary of changes
- Test results (all passing)
- Any trade-offs or limitations
- Next steps (if any)

**Questions to ask:**
- Does the solution meet the original requirements?
- Are there any edge cases we missed?
- Is the code maintainable?
- Should we iterate further or ship?

**3. Update Documentation** üìù

**After all tests pass and human approves**, update relevant documentation:

**Update these locations:**
- `libs/maia-script/src/docs/developers/` - Developer-facing docs
- `libs/maia-script/src/docs/vibecreators/` - Vibecreator-facing docs
- `libs/maia-script/src/docs/getting-started/` - Getting started guides
- Component-level README files (if applicable)

**DO NOT UPDATE:**
- ‚ùå `libs/maia-script/src/docs/agents/LLM_*.md` - These are **auto-generated**

**Documentation should include:**
- What changed and why
- New APIs/interfaces/patterns
- Usage examples
- Migration guide (if breaking changes)

**Output**: 
- Fully tested, working solution
- Human-approved implementation
- Updated documentation
- Ready to ship ‚ú®

**LLM Action**: 
1. Run all tests (unit, integration, browser) and verify success
2. Perform comprehensive browser testing:
   - Navigate to app, take screenshots, check console, verify network requests
   - Document browser test results (screenshots, console logs, network summary)
3. Present results (including browser test results) to human for final approval
4. Update relevant documentation (skip LLM docs)
5. Confirm solution is complete

---

## Planning Output Format

When creating a plan, structure it as follows:

```markdown
---
name: [Short descriptive name]
overview: [One sentence summary]
todos:
  - id: milestone-1
    content: [Milestone 1 description]
    status: pending
  - id: milestone-2
    content: [Milestone 2 description]
    status: pending
---

# [Feature/Intent Name]

## Problem Statement
[Clear definition of what we're solving and why]

## Success Criteria
- **Desirable**: [User benefit]
- **Feasible**: [Technical requirement]
- **Viable**: [Maintainability/sustainability requirement]

## Solution Approach
[Chosen approach with rationale]

## Implementation Milestones

### Milestone 1: [Name]
**Tests to Write:**
- [ ] Unit test for X
- [ ] Integration test for Y

**Implementation:**
- [ ] Create file structure
- [ ] Implement core functionality
- [ ] Ensure tests pass

**Cleanup & Migration:**
- [ ] Remove ALL legacy code replaced by this milestone
- [ ] Delete backwards compatibility hacks
- [ ] Update all call sites to new implementation
- [ ] Verify 100% migration complete

**Browser Testing** (if frontend feature):
- [ ] Navigate to app and take initial screenshot
- [ ] Test user interactions and take screenshots
- [ ] Check browser console for errors/warnings
- [ ] Verify network requests are correct
- [ ] Document browser test results

**Human Checkpoint:** ‚úã Pause for feedback

### Milestone 2: [Name]
**Tests to Write:**
- [ ] Unit test for X
- [ ] Integration test for Y

**Implementation:**
- [ ] Integrate with existing systems
- [ ] Implement feature Z
- [ ] Ensure tests pass

**Cleanup & Migration:**
- [ ] Remove ALL legacy code replaced by this milestone
- [ ] Delete backwards compatibility hacks
- [ ] Update all call sites to new implementation
- [ ] Verify 100% migration complete

**Browser Testing** (if frontend feature):
- [ ] Navigate to app and take initial screenshot
- [ ] Test user interactions and take screenshots
- [ ] Check browser console for errors/warnings
- [ ] Verify network requests are correct
- [ ] Document browser test results

**Human Checkpoint:** ‚úã Pause for feedback

### Milestone 3: Documentation & Final Review
- [ ] All tests passing ‚úÖ (unit, integration, browser)
- [ ] Browser testing complete (screenshots, console, network verified)
- [ ] Update developer docs
- [ ] Update vibecreator docs
- [ ] Final human approval ‚úã

## File Structure
```
[Detailed file/folder structure with new files highlighted]
```

## Testing Strategy
- **Unit Tests**: [What to test and where]
- **Integration Tests**: [What to test and where]
- **Browser Tests**: [What to test in browser - UI, interactions, console, network]
- **Test Output**: [Expected test summary format]
- **Browser Test Output**: [Screenshots, console logs, network requests summary]

## Risks & Mitigation
[Potential issues and how to address them]

## Documentation Updates
- [ ] `docs/developers/[file].md`
- [ ] `docs/vibecreators/[file].md`
- [ ] ‚ùå Skip `docs/agents/LLM_*.md` (auto-generated)
```

## Key Mindsets

**Embrace Ambiguity**: Don't expect perfect clarity upfront. Iterate to refine understanding.

**Reframe**: Challenge assumptions. Ask "What if we approached this differently?"

**Make Tangible**: Convert abstract ideas into concrete plans, file structures, and code.

**Iterate**: Plans are living documents. Revisit stages as new information emerges.

**Be Collaborative**: Consider multiple perspectives (users, developers, maintainers).

**Take Action**: Plans are useless without execution. Balance planning with doing.

## When to Use This Process

- ‚úÖ Planning new features or major refactors
- ‚úÖ Introducing new architectural patterns
- ‚úÖ Solving complex, ill-defined problems
- ‚úÖ When requirements are unclear or evolving

**Skip or simplify** for:
- ‚ùå Trivial changes (typos, simple fixes)
- ‚ùå Well-understood, routine tasks
- ‚ùå When the solution is obvious and low-risk

## Process Flexibility

**Non-Linear**: You can:
- Jump between stages (e.g., implement ‚Üí define if tests reveal misunderstanding)
- Run stages in parallel (ideate while still defining)
- Revisit earlier stages based on new insights
- Skip stages if they're not needed for the specific context
- Iterate within implementation: feedback at each milestone may refine approach

**Iterative**: Plans evolve. Start with a rough sketch, refine through stages, iterate based on feedback.

---

## Example Workflow

1. **User**: "I want to add authentication"
2. **Empathize**: "Who needs auth? What auth patterns exist? What are security requirements?"
3. **Define**: "Problem: Users need secure, scalable auth. Success: OAuth2 support, session management, secure by default."
4. **Ideate**: "Options: OAuth2, JWT, session-based. Recommendation: OAuth2 for flexibility."
5. **Prototype**: "Plan: Add auth module, integrate with kernel, create auth middleware, add tests."
6. **Implement**: 
   - Milestone 1: Write auth module tests ‚Üí Implement auth module ‚Üí Clean up old auth code ‚Üí Human feedback ‚úã
   - Milestone 2: Write integration tests ‚Üí Implement kernel integration ‚Üí Remove legacy patterns ‚Üí Human feedback ‚úã
   - Milestone 3: Write middleware tests ‚Üí Implement middleware ‚Üí Delete all backwards compat layers ‚Üí Human feedback ‚úã
7. **Review & Feedback**: "All tests pass ‚úÖ ‚Üí Human approves ‚Üí Update docs ‚Üí Ship! üöÄ"

---

**Remember**: The goal is not a perfect plan, but a **good enough plan** that can be executed, tested, and refined. Start with "good enough" and iterate toward better.
